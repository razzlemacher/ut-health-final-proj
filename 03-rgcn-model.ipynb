{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513aba00",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76391734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.6.0+cpu\n",
      "torch geometric version:  2.6.1\n",
      "torch cuda version:  None\n",
      "torch cuda available:  False\n",
      "Pandas version: 2.2.3\n",
      "Numpy version: 2.2.4\n",
      "Matplotlib version: 3.10.1\n",
      "NetworkX version: 3.4.2\n",
      "\n",
      "Removed truncation of columns\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch geometric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GraphConv\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "# from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "# from torch_geometric.utils import degree, to_dense_adj, to_dense_batch, to_undirected\n",
    "# from torch_geometric.utils import from_networkx, to_networkx\n",
    "# from torch_geometric.utils import negative_sampling, remove_self_loops, add_self_loops\n",
    "# from torch_geometric.utils import dropout_adj, to_undirected, add_self_loops\n",
    "# from torch_geometric.utils import to_dense_adj, to_dense_batch, dense_to_sparse\n",
    "\n",
    "# print versions\n",
    "print(\"torch version: \", torch.__version__)\n",
    "print(\"torch geometric version: \", torch_geometric.__version__)\n",
    "print(\"torch cuda version: \", torch.version.cuda)\n",
    "print(\"torch cuda available: \", torch.cuda.is_available())\n",
    "# print(\"torch cuda device count: \", torch.cuda.device_count())\n",
    "# print(\"torch cuda current device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# print versions\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "# set pandas display options to show all columns and rows without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"\\nRemoved truncation of columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1434cc4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6acf46",
   "metadata": {},
   "source": [
    "## Load networkx graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph from d:\\Repos\\ut-health-final-proj\\pickle\\max_20000_nodes_graph.gpickle...\n",
      "Graph loaded. Number of nodes: 17654, number of edges: 172728\n"
     ]
    }
   ],
   "source": [
    "# load the networkx graph\n",
    "def load_graph(graph_path):\n",
    "    with open(graph_path, 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    return graph\n",
    "\n",
    "CURR_DIR_PATH = os.getcwd()\n",
    "PICKLE_GRAPH_FILE_NAME = f\"{CURR_DIR_PATH}\\\\pickle\\\\max_20000_nodes_graph.gpickle\"\n",
    "PICKLE_GRAPH_FILE_NAME = os.path.abspath(PICKLE_GRAPH_FILE_NAME)\n",
    "\n",
    "print(f\"Loading graph from {PICKLE_GRAPH_FILE_NAME}...\")\n",
    "ntx_graph = load_graph(PICKLE_GRAPH_FILE_NAME)\n",
    "print(f\"Graph loaded. Number of nodes: {ntx_graph.number_of_nodes()}, number of edges: {ntx_graph.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d981189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node counts: {'patient': 11630, 'diagnosis': 4680, 'procedure': 1344}\n",
      "Edge counts: {'has_diagnosis': 123987, 'has_procedure': 48741}\n"
     ]
    }
   ],
   "source": [
    "# count nodes of type patient, diagnosis and procedure\n",
    "def count_node_types(graph):\n",
    "    node_types = {}\n",
    "    for node in graph.nodes():\n",
    "        node_type = graph.nodes[node]['type']\n",
    "        if node_type not in node_types:\n",
    "            node_types[node_type] = 0\n",
    "        node_types[node_type] += 1\n",
    "    return node_types\n",
    "\n",
    "node_types = count_node_types(ntx_graph)\n",
    "print(f\"Node counts: {node_types}\")\n",
    "\n",
    "# count edges named had_procedure, has_diagnosis\n",
    "def count_edge_types(graph):\n",
    "    edge_types = {}\n",
    "    for u, v, key in graph.edges(keys=True):\n",
    "        edge_type = graph.edges[u, v, key]['relation']\n",
    "        if edge_type not in edge_types:\n",
    "            edge_types[edge_type] = 0\n",
    "        edge_types[edge_type] += 1\n",
    "    return edge_types\n",
    "\n",
    "edge_types = count_edge_types(ntx_graph)\n",
    "print(f\"Edge counts: {edge_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5433ec",
   "metadata": {},
   "source": [
    "## Load patient node similarity dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709c689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading patient similarity dataframe from d:\\Repos\\ut-health-final-proj\\pickle\\max_20000_nodes_similarity.gpickle...\n",
      "Patient similarity dataframe loaded. Number of rows: 1288402\n"
     ]
    }
   ],
   "source": [
    "# load the patient similarity dataframe\n",
    "def load_patient_similarity_df(df_path):\n",
    "    with open(df_path, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    return df\n",
    "\n",
    "CURR_DIR_PATH = os.getcwd()\n",
    "PATIENT_SIMILARITY_DF_FILE_NAME = f\"{CURR_DIR_PATH}\\\\pickle\\\\max_20000_nodes_similarity.gpickle\"\n",
    "PATIENT_SIMILARITY_DF_FILE_NAME = os.path.abspath(PATIENT_SIMILARITY_DF_FILE_NAME)\n",
    "\n",
    "print(f\"Loading patient similarity dataframe from {PATIENT_SIMILARITY_DF_FILE_NAME}...\")\n",
    "patient_similarity_df = load_patient_similarity_df(PATIENT_SIMILARITY_DF_FILE_NAME)\n",
    "print(f\"Patient similarity dataframe loaded. Number of rows: {patient_similarity_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da7142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient1</th>\n",
       "      <th>patient2</th>\n",
       "      <th>patient1_id</th>\n",
       "      <th>patient2_id</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>same_gender</th>\n",
       "      <th>same_age_bucket</th>\n",
       "      <th>is_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>patient-4074</td>\n",
       "      <td>patient-10139</td>\n",
       "      <td>4074</td>\n",
       "      <td>10139</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9670</th>\n",
       "      <td>patient-4074</td>\n",
       "      <td>patient-26572</td>\n",
       "      <td>4074</td>\n",
       "      <td>26572</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>patient-90889</td>\n",
       "      <td>patient-84020</td>\n",
       "      <td>90889</td>\n",
       "      <td>84020</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17072</th>\n",
       "      <td>patient-90889</td>\n",
       "      <td>patient-67648</td>\n",
       "      <td>90889</td>\n",
       "      <td>67648</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19221</th>\n",
       "      <td>patient-90889</td>\n",
       "      <td>patient-6138</td>\n",
       "      <td>90889</td>\n",
       "      <td>6138</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            patient1       patient2  patient1_id  patient2_id  \\\n",
       "423     patient-4074  patient-10139         4074        10139   \n",
       "9670    patient-4074  patient-26572         4074        26572   \n",
       "16410  patient-90889  patient-84020        90889        84020   \n",
       "17072  patient-90889  patient-67648        90889        67648   \n",
       "19221  patient-90889   patient-6138        90889         6138   \n",
       "\n",
       "       jaccard_similarity  same_gender  same_age_bucket  is_similar  \n",
       "423              0.304348        False            False        True  \n",
       "9670             0.300000         True             True        True  \n",
       "16410            0.344828         True            False        True  \n",
       "17072            0.400000        False            False        True  \n",
       "19221            0.333333         True            False        True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_similarity_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_node</th>\n",
       "      <th>target_node</th>\n",
       "      <th>patient1_id</th>\n",
       "      <th>patient2_id</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>same_gender</th>\n",
       "      <th>same_age_bucket</th>\n",
       "      <th>is_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>patient-4074</td>\n",
       "      <td>patient-10139</td>\n",
       "      <td>4074</td>\n",
       "      <td>10139</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9670</th>\n",
       "      <td>patient-4074</td>\n",
       "      <td>patient-26572</td>\n",
       "      <td>4074</td>\n",
       "      <td>26572</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>patient-90889</td>\n",
       "      <td>patient-84020</td>\n",
       "      <td>90889</td>\n",
       "      <td>84020</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17072</th>\n",
       "      <td>patient-90889</td>\n",
       "      <td>patient-67648</td>\n",
       "      <td>90889</td>\n",
       "      <td>67648</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19221</th>\n",
       "      <td>patient-90889</td>\n",
       "      <td>patient-6138</td>\n",
       "      <td>90889</td>\n",
       "      <td>6138</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source_node    target_node  patient1_id  patient2_id  \\\n",
       "423     patient-4074  patient-10139         4074        10139   \n",
       "9670    patient-4074  patient-26572         4074        26572   \n",
       "16410  patient-90889  patient-84020        90889        84020   \n",
       "17072  patient-90889  patient-67648        90889        67648   \n",
       "19221  patient-90889   patient-6138        90889         6138   \n",
       "\n",
       "       jaccard_similarity  same_gender  same_age_bucket  is_similar  \n",
       "423              0.304348        False            False        True  \n",
       "9670             0.300000         True             True        True  \n",
       "16410            0.344828         True            False        True  \n",
       "17072            0.400000        False            False        True  \n",
       "19221            0.333333         True            False        True  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename column patient1 to source and patient2 to target\n",
    "patient_similarity_df.rename(columns={'patient1': 'source_node', 'patient2': 'target_node'}, inplace=True)\n",
    "patient_similarity_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b07df0",
   "metadata": {},
   "source": [
    "# Create R-GCN and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798fed9",
   "metadata": {},
   "source": [
    "### DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_types: {'patient': [], 'diagnosis': [], 'procedure': []}\n",
      "node_mappings: {'patient': {}, 'diagnosis': {}, 'procedure': {}}\n",
      "node: patient-4074\n",
      "attr: {'gender': 'M', 'age_bucket': 80, 'hadm_id': 137421, 'type': 'patient'}\n",
      "node_type: patient\n",
      "node_mappings[patient][patient-4074]: 0\n",
      "node_mappings[node_type][node]: 0\n",
      "node_mappings[patient][patient-90889]: 1\n",
      "node_mappings[node_type][node]: 1\n",
      "node_mappings[patient][patient-72753]: 2\n",
      "node_mappings[node_type][node]: 2\n"
     ]
    }
   ],
   "source": [
    "# Get node types\n",
    "node_types = {node_type: [] for node_type in ['patient', 'diagnosis', 'procedure']}\n",
    "print(f\"node_types: {node_types}\")\n",
    "\n",
    "    \n",
    "# Create mappings from original node IDs to new indices for each node type\n",
    "node_mappings = {node_type: {} for node_type in node_types}\n",
    "\n",
    "print(f\"node_mappings: {node_mappings}\")\n",
    "i=0\n",
    "for node, attr in ntx_graph.nodes(data=True):\n",
    "    node_type = attr['type']\n",
    "    if i ==0:\n",
    "        print(f\"node: {node}\")\n",
    "        print(f\"attr: {attr}\")\n",
    "        print(f\"node_type: {node_type}\")\n",
    "\n",
    "    if node not in node_mappings[node_type]:\n",
    "        node_mappings[node_type][node] = len(node_mappings[node_type])\n",
    "\n",
    "        i+=1\n",
    "        print(f\"node_mappings[{node_type}][{node}]: {node_mappings[node_type][node]}\")\n",
    "        print(f\"node_mappings[node_type][node]: {node_mappings[node_type][node]}\")\n",
    "\n",
    "    if i==3:\n",
    "        break\n",
    "\n",
    "# PROMPT:\n",
    "# Create a relational graph neural network for contrastive learning. The objective is to predict patient similarity. \n",
    "# The data is in the networkx graph named ntx_graph. It contains 3 types of nodes: patients, diagnosis and procedure.\n",
    "# It has edges with the relation of has_diagnosis and had_procedure. \n",
    "# The dataframe patient_similarity_df contains patient similarity score in the column \"jaccard_similarity\". It has source node in column source_node and target node in column target_node. The similar rows have the column is_similar=True and dissimilar rows have is_similar=False.\n",
    "# Create a relational graph neural network that uses the training data derived from ntx_graph and validation data derived from patient_similarity_df.\n",
    "# Train and validate the model and print the loss in every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001deb1",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17c1ab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting NetworkX graph to HeteroData...\n",
      "Processing similarity data...\n",
      "Created 50000 training pairs from 50000 samples.\n",
      "Using device: cpu\n",
      "Training model...\n",
      "Epoch: 010, Loss: 0.1938\n",
      "Epoch: 020, Loss: 0.0502\n",
      "Epoch: 030, Loss: 0.0296\n",
      "Epoch: 040, Loss: 0.0219\n",
      "Epoch: 050, Loss: 0.0174\n",
      "Epoch: 060, Loss: 0.0152\n",
      "Epoch: 070, Loss: 0.0136\n",
      "Epoch: 080, Loss: 0.0124\n",
      "Epoch: 090, Loss: 0.0116\n",
      "Epoch: 100, Loss: 0.0111\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GraphConv, HeteroConv\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# Convert NetworkX graph to PyTorch Geometric HeteroData\n",
    "def convert_to_hetero_data(nx_graph):\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Define node types\n",
    "    node_types_list = ['patient', 'diagnosis', 'procedure']\n",
    "    \n",
    "    # Create mappings from original node IDs to new indices for each node type\n",
    "    node_mappings = {node_type: {} for node_type in node_types_list}\n",
    "\n",
    "    # Populate node mappings\n",
    "    for node, attr in nx_graph.nodes(data=True):\n",
    "        node_type = attr['type']\n",
    "        if node_type in node_mappings:\n",
    "            if node not in node_mappings[node_type]:\n",
    "                node_mappings[node_type][node] = len(node_mappings[node_type])\n",
    "    \n",
    "    # Add node features (using one-hot encoding)\n",
    "    for node_type, mapping in node_mappings.items():\n",
    "        num_nodes = len(mapping)\n",
    "        if num_nodes > 0:\n",
    "            data[node_type].x = torch.eye(num_nodes)\n",
    "        else:\n",
    "            # Handle case where a node type might have 0 nodes\n",
    "            data[node_type].x = torch.empty((0, 0), dtype=torch.float)\n",
    "\n",
    "    # Define edge types based on relations found in the graph\n",
    "    edge_types_relations = {\n",
    "        'has_diagnosis': ('patient', 'has_diagnosis', 'diagnosis'),\n",
    "        'has_procedure': ('patient', 'has_procedure', 'procedure')\n",
    "    }\n",
    "\n",
    "    # Initialize edge index storage\n",
    "    for src_type, rel, dst_type in edge_types_relations.values():\n",
    "         data[src_type, rel, dst_type].edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    # Add edges\n",
    "    edge_indices_dict = {rel: [[], []] for rel in edge_types_relations.keys()}\n",
    "\n",
    "    for u, v, key, attr in nx_graph.edges(keys=True, data=True):\n",
    "        relation = attr.get('relation')\n",
    "        if relation in edge_types_relations:\n",
    "            src_type, _, dst_type = edge_types_relations[relation]\n",
    "            \n",
    "            # Determine the correct source and destination based on node types\n",
    "            u_type = nx_graph.nodes[u]['type']\n",
    "            v_type = nx_graph.nodes[v]['type']\n",
    "\n",
    "            # Map original node IDs to new indices\n",
    "            if u_type == src_type and v_type == dst_type:\n",
    "                if u in node_mappings[src_type] and v in node_mappings[dst_type]:\n",
    "                    src_idx = node_mappings[src_type][u]\n",
    "                    dst_idx = node_mappings[dst_type][v]\n",
    "                    edge_indices_dict[relation][0].append(src_idx)\n",
    "                    edge_indices_dict[relation][1].append(dst_idx)\n",
    "            elif u_type == dst_type and v_type == src_type: # Handle potential reverse direction in source data\n",
    "                 if v in node_mappings[src_type] and u in node_mappings[dst_type]:\n",
    "                    src_idx = node_mappings[src_type][v]\n",
    "                    dst_idx = node_mappings[dst_type][u]\n",
    "                    edge_indices_dict[relation][0].append(src_idx)\n",
    "                    edge_indices_dict[relation][1].append(dst_idx)\n",
    "\n",
    "\n",
    "    # Assign edge indices to the HeteroData object\n",
    "    for relation, (src_type, rel_key, dst_type) in edge_types_relations.items():\n",
    "        indices = torch.tensor(edge_indices_dict[relation], dtype=torch.long)\n",
    "        # Ensure edge_index is shape (2, num_edges) even if empty\n",
    "        data[src_type, rel_key, dst_type].edge_index = indices.view(2, -1)\n",
    "\n",
    "    return data, node_mappings\n",
    "\n",
    "# Create RGCN model for contrastive learning\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dynamically create convolutions based on metadata\n",
    "        conv_dict1 = {}\n",
    "        conv_dict2 = {}\n",
    "        \n",
    "        for edge_type in metadata[1]:\n",
    "            # edge_type is ('src_node_type', 'relation', 'dst_node_type')\n",
    "            # Use -1 for input channels to infer automatically\n",
    "            conv_dict1[edge_type] = GraphConv(-1, hidden_channels)\n",
    "            conv_dict2[edge_type] = GraphConv(hidden_channels, hidden_channels)\n",
    "            \n",
    "        self.conv1 = torch_geometric.nn.HeteroConv(conv_dict1, aggr='sum')\n",
    "        self.conv2 = torch_geometric.nn.HeteroConv(conv_dict2, aggr='sum')\n",
    "        \n",
    "        # Projection head specifically for patient embeddings\n",
    "        # The input size to the linear layer depends on the output of conv2 for 'patient' nodes\n",
    "        self.patient_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels), # Adjust input size if needed\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # First layer\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Second layer\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        # No ReLU after the second layer before projection\n",
    "        \n",
    "        # Project patient embeddings for contrastive learning\n",
    "        # Check if 'patient' key exists before projecting\n",
    "        if 'patient' in x_dict:\n",
    "            patient_emb = self.patient_proj(x_dict['patient'])\n",
    "            return patient_emb\n",
    "        else:\n",
    "            # Handle cases where 'patient' embeddings might not be produced (e.g., graph structure)\n",
    "            return None # Or raise an error, or return an empty tensor\n",
    "\n",
    "# Create a contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, embeddings, pairs, labels):\n",
    "        # Ensure pairs and labels are on the same device as embeddings\n",
    "        pairs = pairs.to(embeddings.device)\n",
    "        labels = labels.to(embeddings.device)\n",
    "\n",
    "        # Extract embeddings for pairs\n",
    "        # Ensure indices in pairs are within the bounds of embeddings\n",
    "        valid_indices_mask = (pairs[:, 0] < embeddings.size(0)) & (pairs[:, 1] < embeddings.size(0))\n",
    "        valid_pairs = pairs[valid_indices_mask]\n",
    "        valid_labels = labels[valid_indices_mask]\n",
    "\n",
    "        if valid_pairs.size(0) == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True) # Return zero loss if no valid pairs\n",
    "\n",
    "        embeddings1 = embeddings[valid_pairs[:, 0]]\n",
    "        embeddings2 = embeddings[valid_pairs[:, 1]]\n",
    "        \n",
    "        # Calculate Euclidean distance\n",
    "        distances = F.pairwise_distance(embeddings1, embeddings2)\n",
    "        \n",
    "        # Calculate loss based on labels (0 for similar, 1 for dissimilar)\n",
    "        # Loss for similar pairs (label=0): distance^2\n",
    "        # Loss for dissimilar pairs (label=1): max(0, margin - distance)^2\n",
    "        loss_similar = (1 - valid_labels) * torch.pow(distances, 2)\n",
    "        loss_dissimilar = valid_labels * torch.pow(torch.clamp(self.margin - distances, min=0.0), 2)\n",
    "        \n",
    "        loss = loss_similar + loss_dissimilar\n",
    "        return loss.mean()\n",
    "\n",
    "# Process NetworkX graph\n",
    "print(\"Converting NetworkX graph to HeteroData...\")\n",
    "data, node_mappings = convert_to_hetero_data(ntx_graph)\n",
    "\n",
    "# Add reverse edges to make the graph effectively undirected for message passing\n",
    "transform = T.ToUndirected()\n",
    "data = transform(data)\n",
    "\n",
    "# Create a lookup from original patient node ID to new sequential index\n",
    "patient_node_lookup = {node: idx for node, idx in node_mappings.get('patient', {}).items()}\n",
    "\n",
    "# Process similarity data for training\n",
    "print(\"Processing similarity data...\")\n",
    "sim_pairs_list = []\n",
    "sim_labels_list = []\n",
    "\n",
    "# Consider using a subset for faster training/debugging if the dataset is large\n",
    "# Determine sample size (e.g., 50k or full dataset)\n",
    "sample_size = min(50000, len(patient_similarity_df))\n",
    "# Use random sampling if taking a subset\n",
    "subset_df = patient_similarity_df.sample(n=sample_size, random_state=42) if sample_size < len(patient_similarity_df) else patient_similarity_df\n",
    "\n",
    "\n",
    "for _, row in subset_df.iterrows():\n",
    "    source = row['source_node']\n",
    "    target = row['target_node']\n",
    "    # Check if both source and target patients are in our mapping\n",
    "    if source in patient_node_lookup and target in patient_node_lookup:\n",
    "        source_idx = patient_node_lookup[source]\n",
    "        target_idx = patient_node_lookup[target]\n",
    "        sim_pairs_list.append([source_idx, target_idx])\n",
    "        # Use 0 for similar, 1 for dissimilar for the contrastive loss formula used\n",
    "        sim_labels_list.append(0.0 if row['is_similar'] else 1.0)\n",
    "\n",
    "if not sim_pairs_list:\n",
    "     raise ValueError(\"No valid training pairs found. Check patient IDs in similarity data and graph.\")\n",
    "\n",
    "sim_pairs = torch.tensor(sim_pairs_list, dtype=torch.long)\n",
    "sim_labels = torch.tensor(sim_labels_list, dtype=torch.float)\n",
    "\n",
    "print(f\"Created {len(sim_pairs)} training pairs from {sample_size} samples.\")\n",
    "\n",
    "# --- Model, Optimizer, and Training Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure data is on the correct device\n",
    "data = data.to(device)\n",
    "sim_pairs = sim_pairs.to(device)\n",
    "sim_labels = sim_labels.to(device)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "hidden_channels = 64\n",
    "out_channels = 32\n",
    "model = RGCN(hidden_channels, out_channels, data.metadata()).to(device)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4) # Added weight decay\n",
    "criterion = ContrastiveLoss(margin=1.0) # Adjusted margin\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get patient embeddings\n",
    "    patient_embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "    \n",
    "    if patient_embeddings is None or patient_embeddings.size(0) == 0:\n",
    "         print(\"Warning: No patient embeddings generated.\")\n",
    "         return 0.0 # Or handle appropriately\n",
    "\n",
    "    # Compute loss using only the patient embeddings\n",
    "    loss = criterion(patient_embeddings, sim_pairs, sim_labels)\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    else:\n",
    "        # Handle cases where loss does not require gradients (e.g., no valid pairs)\n",
    "        print(\"Warning: Loss does not require gradients.\")\n",
    "        return loss.item() # Return the scalar value\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "num_epochs = 100 # Increased epochs\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train()\n",
    "    if (epoch + 1) % 10 == 0: # Print loss every 10 epochs\n",
    "        print(f'Epoch: {epoch+1:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc682831",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8367a3b",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "456a8352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to d:\\Repos\\ut-health-final-proj\\models\\rgcn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# save the model to a file\n",
    "MODEL_FILE_NAME = f\"{CURR_DIR_PATH}\\\\models\\\\rgcn_model.pth\"\n",
    "torch.save(model.state_dict(), MODEL_FILE_NAME)\n",
    "print(f\"Model saved to {MODEL_FILE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99097fe",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4df7411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from d:\\Repos\\ut-health-final-proj\\models\\rgcn_model.pth\n",
      "Model Summary:\n",
      "RGCN(\n",
      "  (conv1): HeteroConv(num_relations=4)\n",
      "  (conv2): HeteroConv(num_relations=4)\n",
      "  (patient_proj): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "Output Data:\n",
      "patient_embeddings: tensor([[-0.2149, -0.3542,  0.1598,  ..., -0.8986, -0.2081, -0.7109],\n",
      "        [-0.2317, -0.0439,  0.0421,  ..., -0.2855,  0.0356, -0.1715],\n",
      "        [-0.1430, -0.2358,  0.1536,  ..., -0.4589, -0.0924, -0.4197],\n",
      "        ...,\n",
      "        [-0.6093, -0.2219,  0.1109,  ..., -1.7633, -0.8697, -1.2796],\n",
      "        [-0.3326, -0.2799,  0.2922,  ..., -0.9005, -0.1450, -0.5438],\n",
      "        [-0.2193, -0.0496,  0.0270,  ..., -0.2097,  0.0538, -0.0965]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Model Summary Finished.\n",
      "Model summary printed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the model from a file\n",
    "def load_model(model, file_path):\n",
    "    model.load_state_dict(torch.load(file_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# load the model from a file\n",
    "model = RGCN(hidden_channels, out_channels, data.metadata()).to(device)\n",
    "model = load_model(model, MODEL_FILE_NAME)\n",
    "print(f\"Model loaded from {MODEL_FILE_NAME}\")\n",
    "\n",
    "\n",
    "# print model summary\n",
    "def print_model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    print(model)\n",
    "    # print(\"\\nModel Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(f\"{name}: {param.data.size()}\")\n",
    "    # print(\"\\nModel Forward Pass:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(f\"{name}: {param.data.size()}\")\n",
    "    # print(\"\\nModel Forward Pass:\")\n",
    "    # print(\"Input Data:\")\n",
    "    # print(f\"data.x_dict: {data.x_dict}\")\n",
    "    # print(f\"data.edge_index_dict: {data.edge_index_dict}\")\n",
    "    print(\"Output Data:\")\n",
    "    print(f\"patient_embeddings: {model(data.x_dict, data.edge_index_dict)}\")\n",
    "    print(\"\\nModel Summary Finished.\")\n",
    "\n",
    "print_model_summary(model)\n",
    "# print model summary\n",
    "print(\"Model summary printed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
